## Урок 5. Scrapy

Найдите сайт, содержащи й интересующий вас список или каталог. Это может быть список книг, фильмов, спортивных
команд или что-то еще, что вас заинтересовало.

Создайте новый проект Scrapy и определите нового паука. С помощью атрибута start_urls укажите URL выбранной 
вами веб-страницы.

Определите метод парсинга для извлечения интересующих вас данных. Используйте селекторы XPath или CSS для 
навигации по HTML и извлечения данных. Возможно, потребуется извлечь данные с нескольких страниц или перейти
по ссылкам на другие страницы.

Сохраните извлеченные данные в структурированном формате. Вы можете использовать оператор yield для возврата
данных из паука, которые Scrapy может записать в файл в выбранном вами формате (например, JSON или CSV).

Конечным результатом работы должен быть код Scrapy Spider, а также пример выходных данных. Не забывайте
соблюдать правила robots.txt и условия обслуживания веб-сайта, а также ответственно подходите к использованию
веб-скрейпинга.

### Описание решения

Код паука представлен в [bonds.py](scrapy%2Fbondsparcer%2Fspiders%2Fbonds.py)

Результат работы паук записывает в файл [output.json](scrapy%2Fbondsparcer%2Foutput.json)

Сохранение результата работы паука настроено в файле [settings.py](scrapy%2Fbondsparcer%2Fsettings.py) через параметры

```commandline
FEED_FORMAT = 'json'   # Устанавливаем формат вывода как JSON
FEED_URI = 'output.json'  # Указываем путь к файлу, куда будут сохраняться данные
```
В файле [pipelines.py](scrapy%2Fbondsparcer%2Fpipelines.py) в методе ` process_item` при необходимости можно настроить обработку 
спарсенных данных.

Для парсинга используется селектор XPath, при парсинге осуществляется переход по страницам за счет рекурсивного вызова 
метода `parse`.  

---

## Урок 4. Парсинг HTML. XPath

Выберите веб-сайт с табличными данными, который вас интересует.
Напишите код Python, использующий библиотеку requests для отправки HTTP GET-запроса на сайт и получения HTML-содержимого страницы.
Выполните парсинг содержимого HTML с помощью библиотеки lxml, чтобы извлечь данные из таблицы.
Сохраните извлеченные данные в CSV-файл с помощью модуля csv.

[xpaths.py](xpaths.py) код для загрузки данных

[data.csv](data.csv) результат работы скрипта

---

## Урок 3. Системы управления базами данных MongoDB и Кликхаус в Python

Установите MongoDB на локальной машине, а также зарегистрируйтесь в онлайн-сервисе. https://www.mongodb.com/ https://www.mongodb.com/products/compass

Регистрация в онлайнсервисе
![2024-11-23_11-22-32.png](2024-11-23_11-22-32.png) 


Загрузите данные, которые вы получили на предыдущем уроке путем скрейпинга сайта с помощью Buautiful Soup в MongoDB и создайте базу данных и коллекции для их хранения.
Поэкспериментируйте с различными методами запросов.

[mongodb.py](mongodb.py) код по работе с MongoDB (развернута с помощью Docker)

Зарегистрируйтесь в ClickHouse.
Загрузите данные в ClickHouse и создайте таблицу для их хранения.

[clickhouse.py](clickhouse.py) код для вставки данных в таблицу ClickHouse

![2024-11-23_12-07-48.png](2024-11-23_12-07-48.png) результат работы скрипта

---

## Урок 2. Парсинг HTML. BeautifulSoup

[BeautifulSoup.py](BeautifulSoup.py) код второго урока

[books_upc.json](books_upc.json) результат работы

